{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03596023-9768-4e5c-a1f2-e483941faa18",
   "metadata": {},
   "source": [
    "# soal 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46777c5e-6897-4eb8-8a54-8ed3cf4b2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# natonestam data ro download konam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1e1b8fa-96d9-4c83-9ab1-2e8d253fff72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.33</td>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.35</td>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.93</td>\n",
       "      <td>118220.22</td>\n",
       "      <td>794.70</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>130.50</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>8042.21</td>\n",
       "      <td>103.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.08</td>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>72.58</td>\n",
       "      <td>5811.16</td>\n",
       "      <td>5677.40</td>\n",
       "      <td>133.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.28</td>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>5986.26</td>\n",
       "      <td>197.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AveragePrice  Total Volume     4046       4225    4770  Total Bags  \\\n",
       "0          1.33      64236.62  1036.74   54454.85   48.16     8696.87   \n",
       "1          1.35      54876.98   674.28   44638.81   58.33     9505.56   \n",
       "2          0.93     118220.22   794.70  109149.67  130.50     8145.35   \n",
       "3          1.08      78992.15  1132.00   71976.41   72.58     5811.16   \n",
       "4          1.28      51039.60   941.48   43838.39   75.78     6183.95   \n",
       "\n",
       "   Small Bags  Large Bags  XLarge Bags          type  year  region  \n",
       "0     8603.62       93.25          0.0  conventional  2015  Albany  \n",
       "1     9408.07       97.49          0.0  conventional  2015  Albany  \n",
       "2     8042.21      103.14          0.0  conventional  2015  Albany  \n",
       "3     5677.40      133.76          0.0  conventional  2015  Albany  \n",
       "4     5986.26      197.69          0.0  conventional  2015  Albany  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('avocado.csv')\n",
    "df.drop('Date', axis = 1, inplace = True)\n",
    "df.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09bcf422-3ea5-4af3-8ae3-58ae280f1223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc6a9d03-1f7d-419f-b6b9-947134b3599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop('AveragePrice', axis = 1)\n",
    "y = df.AveragePrice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d3e44b7-18f5-4a5a-8b5e-3078e5724a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a0fbfbb-def2-41ea-9c9e-5e66589397c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.06748798845823227\n",
      "R-squared: 0.5828058108767373\n",
      "Mean Absolute Error (MAE): 0.19443438060128107\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 71.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sickit learn regression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('avocado.csv')\n",
    "df.drop('Date', axis = 1, inplace = True)\n",
    "df.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "# Prepare the data\n",
    "X = df.drop('AveragePrice', axis = 1) # feature\n",
    "X = pd.get_dummies(X, columns = ['type','year', 'region'])\n",
    "y = df.AveragePrice # target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3803e53a-e5a5-48fc-9fcc-bdb9ad2b6c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "Mean Absolute Error (MAE): 1.4059784097758825\n",
      "R-squared (R²): -12.191816356693662\n",
      "CPU times: total: 15.7 s\n",
      "Wall time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('avocado.csv')\n",
    "df.drop('Date', axis=1, inplace=True)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# Prepare the data\n",
    "X = df.drop('AveragePrice', axis=1)  # feature\n",
    "X = pd.get_dummies(X, columns=['type', 'year', 'region'])\n",
    "y = df.AveragePrice.values  # target\n",
    "\n",
    "# Convert all data to numeric types\n",
    "X = X.apply(pd.to_numeric)\n",
    "y = y.astype(float)\n",
    "\n",
    "# Add an intercept term\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
    "\n",
    "#Mean Absolute Error\n",
    "def mean_absolute_error_loss(beta, X, y):\n",
    "    y_pred = X @ beta\n",
    "    return np.mean(np.abs(y - y_pred))\n",
    "\n",
    "# Gradient descent function\n",
    "def gradient_descent(X, y, beta, learning_rate, iterations):\n",
    "    m = len(y)\n",
    "    for _ in range(iterations):\n",
    "        y_pred = X @ beta\n",
    "        gradient = np.sign(y_pred - y) @ X / m\n",
    "        beta = beta - (learning_rate * gradient)\n",
    "        if (_+1) % 100 == 0: print(_+1)\n",
    "    return beta\n",
    "\n",
    "# Initialize parameters\n",
    "initial_beta = np.zeros(X.shape[1])\n",
    "learning_rate = 0.01\n",
    "iterations = 500\n",
    "\n",
    "# Optimize using gradient descent\n",
    "beta_optimized = gradient_descent(X, y, initial_beta, learning_rate, iterations)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = X @ beta_optimized\n",
    "\n",
    "# Evaluate the model\n",
    "mae = np.mean(np.abs(y - y_pred))\n",
    "ss_total = np.sum((y - np.mean(y)) ** 2)\n",
    "ss_residual = np.sum((y - y_pred) ** 2)\n",
    "r2 = 1 - (ss_residual / ss_total)\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'R-squared (R²): {r2}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d0e0b-3bb3-409c-9664-b5739010a739",
   "metadata": {},
   "source": [
    "# ghesmat 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35e1f37-9d8a-4cce-b8c6-5fa1c1e42ff3",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bc7ce63-184e-44b7-8214-ec24f2686c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6467b7b-758c-4e10-9c0b-9ae943fa332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('OnlineNewsPopularity.csv')\n",
    "df.drop('url', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b1bb3cb-ef0e-4cac-8622-37c70d8c6799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "0      731.0            12.0             219.0         0.663594   \n",
       "1      731.0             9.0             255.0         0.604743   \n",
       "2      731.0             9.0             211.0         0.575130   \n",
       "3      731.0             9.0             531.0         0.503788   \n",
       "4      731.0            13.0            1072.0         0.415646   \n",
       "\n",
       "   n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  \\\n",
       "0               1.0                  0.815385        4.0             2.0   \n",
       "1               1.0                  0.791946        3.0             1.0   \n",
       "2               1.0                  0.663866        3.0             1.0   \n",
       "3               1.0                  0.665635        9.0             0.0   \n",
       "4               1.0                  0.540890       19.0            19.0   \n",
       "\n",
       "   num_imgs  num_videos  ...  min_positive_polarity  max_positive_polarity  \\\n",
       "0       1.0         0.0  ...               0.100000                    0.7   \n",
       "1       1.0         0.0  ...               0.033333                    0.7   \n",
       "2       1.0         0.0  ...               0.100000                    1.0   \n",
       "3       1.0         0.0  ...               0.136364                    0.8   \n",
       "4      20.0         0.0  ...               0.033333                    1.0   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.350000                 -0.600              -0.200000   \n",
       "1              -0.118750                 -0.125              -0.100000   \n",
       "2              -0.466667                 -0.800              -0.133333   \n",
       "3              -0.369697                 -0.600              -0.166667   \n",
       "4              -0.220192                 -0.500              -0.050000   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0            0.500000                 -0.187500                0.000000   \n",
       "1            0.000000                  0.000000                0.500000   \n",
       "2            0.000000                  0.000000                0.500000   \n",
       "3            0.000000                  0.000000                0.500000   \n",
       "4            0.454545                  0.136364                0.045455   \n",
       "\n",
       "   abs_title_sentiment_polarity  shares  \n",
       "0                      0.187500     593  \n",
       "1                      0.000000     711  \n",
       "2                      0.000000    1500  \n",
       "3                      0.000000    1200  \n",
       "4                      0.136364     505  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cacd4fa0-e4ce-4bbc-9b79-ad008ca503a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>354.530471</td>\n",
       "      <td>10.398749</td>\n",
       "      <td>546.514731</td>\n",
       "      <td>0.548216</td>\n",
       "      <td>0.996469</td>\n",
       "      <td>0.689175</td>\n",
       "      <td>10.883690</td>\n",
       "      <td>3.293638</td>\n",
       "      <td>4.544143</td>\n",
       "      <td>1.249874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095446</td>\n",
       "      <td>0.756728</td>\n",
       "      <td>-0.259524</td>\n",
       "      <td>-0.521944</td>\n",
       "      <td>-0.107500</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.071425</td>\n",
       "      <td>0.341843</td>\n",
       "      <td>0.156064</td>\n",
       "      <td>3395.380184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>214.163767</td>\n",
       "      <td>2.114037</td>\n",
       "      <td>471.107508</td>\n",
       "      <td>3.520708</td>\n",
       "      <td>5.231231</td>\n",
       "      <td>3.264816</td>\n",
       "      <td>11.332017</td>\n",
       "      <td>3.855141</td>\n",
       "      <td>8.309434</td>\n",
       "      <td>4.107855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071315</td>\n",
       "      <td>0.247786</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>0.290290</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>0.324247</td>\n",
       "      <td>0.265450</td>\n",
       "      <td>0.188791</td>\n",
       "      <td>0.226294</td>\n",
       "      <td>11626.950749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>164.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.470870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625739</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.328383</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>946.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>339.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>0.539226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.253333</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>542.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.754630</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.186905</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>8474.000000</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>1042.000000</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "count  39644.000000    39644.000000      39644.000000     39644.000000   \n",
       "mean     354.530471       10.398749        546.514731         0.548216   \n",
       "std      214.163767        2.114037        471.107508         3.520708   \n",
       "min        8.000000        2.000000          0.000000         0.000000   \n",
       "25%      164.000000        9.000000        246.000000         0.470870   \n",
       "50%      339.000000       10.000000        409.000000         0.539226   \n",
       "75%      542.000000       12.000000        716.000000         0.608696   \n",
       "max      731.000000       23.000000       8474.000000       701.000000   \n",
       "\n",
       "       n_non_stop_words  n_non_stop_unique_tokens     num_hrefs  \\\n",
       "count      39644.000000              39644.000000  39644.000000   \n",
       "mean           0.996469                  0.689175     10.883690   \n",
       "std            5.231231                  3.264816     11.332017   \n",
       "min            0.000000                  0.000000      0.000000   \n",
       "25%            1.000000                  0.625739      4.000000   \n",
       "50%            1.000000                  0.690476      8.000000   \n",
       "75%            1.000000                  0.754630     14.000000   \n",
       "max         1042.000000                650.000000    304.000000   \n",
       "\n",
       "       num_self_hrefs      num_imgs    num_videos  ...  min_positive_polarity  \\\n",
       "count    39644.000000  39644.000000  39644.000000  ...           39644.000000   \n",
       "mean         3.293638      4.544143      1.249874  ...               0.095446   \n",
       "std          3.855141      8.309434      4.107855  ...               0.071315   \n",
       "min          0.000000      0.000000      0.000000  ...               0.000000   \n",
       "25%          1.000000      1.000000      0.000000  ...               0.050000   \n",
       "50%          3.000000      1.000000      0.000000  ...               0.100000   \n",
       "75%          4.000000      4.000000      1.000000  ...               0.100000   \n",
       "max        116.000000    128.000000     91.000000  ...               1.000000   \n",
       "\n",
       "       max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
       "count           39644.000000           39644.000000           39644.000000   \n",
       "mean                0.756728              -0.259524              -0.521944   \n",
       "std                 0.247786               0.127726               0.290290   \n",
       "min                 0.000000              -1.000000              -1.000000   \n",
       "25%                 0.600000              -0.328383              -0.700000   \n",
       "50%                 0.800000              -0.253333              -0.500000   \n",
       "75%                 1.000000              -0.186905              -0.300000   \n",
       "max                 1.000000               0.000000               0.000000   \n",
       "\n",
       "       max_negative_polarity  title_subjectivity  title_sentiment_polarity  \\\n",
       "count           39644.000000        39644.000000              39644.000000   \n",
       "mean               -0.107500            0.282353                  0.071425   \n",
       "std                 0.095373            0.324247                  0.265450   \n",
       "min                -1.000000            0.000000                 -1.000000   \n",
       "25%                -0.125000            0.000000                  0.000000   \n",
       "50%                -0.100000            0.150000                  0.000000   \n",
       "75%                -0.050000            0.500000                  0.150000   \n",
       "max                 0.000000            1.000000                  1.000000   \n",
       "\n",
       "       abs_title_subjectivity  abs_title_sentiment_polarity         shares  \n",
       "count            39644.000000                  39644.000000   39644.000000  \n",
       "mean                 0.341843                      0.156064    3395.380184  \n",
       "std                  0.188791                      0.226294   11626.950749  \n",
       "min                  0.000000                      0.000000       1.000000  \n",
       "25%                  0.166667                      0.000000     946.000000  \n",
       "50%                  0.500000                      0.000000    1400.000000  \n",
       "75%                  0.500000                      0.250000    2800.000000  \n",
       "max                  0.500000                      1.000000  843300.000000  \n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85ed713d-011e-4f31-87bf-ea6648dde3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()\n",
    "# means no missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bbc7466b-a431-40f6-9bdc-7ff8d14420a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([' timedelta', ' n_tokens_title', ' n_tokens_content',\n",
       "       ' n_unique_tokens', ' n_non_stop_words', ' n_non_stop_unique_tokens',\n",
       "       ' num_hrefs', ' num_self_hrefs', ' num_imgs', ' num_videos',\n",
       "       ' average_token_length', ' num_keywords', ' data_channel_is_lifestyle',\n",
       "       ' data_channel_is_entertainment', ' data_channel_is_bus',\n",
       "       ' data_channel_is_socmed', ' data_channel_is_tech',\n",
       "       ' data_channel_is_world', ' kw_min_min', ' kw_max_min', ' kw_avg_min',\n",
       "       ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg',\n",
       "       ' kw_max_avg', ' kw_avg_avg', ' self_reference_min_shares',\n",
       "       ' self_reference_max_shares', ' self_reference_avg_sharess',\n",
       "       ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday',\n",
       "       ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday',\n",
       "       ' weekday_is_sunday', ' is_weekend', ' LDA_00', ' LDA_01', ' LDA_02',\n",
       "       ' LDA_03', ' LDA_04', ' global_subjectivity',\n",
       "       ' global_sentiment_polarity', ' global_rate_positive_words',\n",
       "       ' global_rate_negative_words', ' rate_positive_words',\n",
       "       ' rate_negative_words', ' avg_positive_polarity',\n",
       "       ' min_positive_polarity', ' max_positive_polarity',\n",
       "       ' avg_negative_polarity', ' min_negative_polarity',\n",
       "       ' max_negative_polarity', ' title_subjectivity',\n",
       "       ' title_sentiment_polarity', ' abs_title_subjectivity',\n",
       "       ' abs_title_sentiment_polarity', ' shares'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3c7e49b4-035a-40cd-837b-3e3d113d194a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "        ... \n",
       "39639    0.0\n",
       "39640    0.0\n",
       "39641    0.0\n",
       "39642    1.0\n",
       "39643    0.0\n",
       "Name:  data_channel_is_world, Length: 39644, dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,17]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f5ba9-eda6-424c-b0f6-a63097c76364",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a75f5f-1729-41ff-849d-8e82fb89c54b",
   "metadata": {},
   "source": [
    "## t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "64a7e923-8a8d-4dbd-b26b-3f115e613037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f4d194b1-6e4d-4e04-a27c-bae17eaf0951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: 3.3769109636398387, P-value: 0.0007337519086551708\n",
      "Reject the null hypothesis: There is a significant difference between the means.\n"
     ]
    }
   ],
   "source": [
    "# weekend vs weekday\n",
    "\n",
    "# shares on weekend vs not on weekends\n",
    "weekend = df.loc[df.iloc[:,37]==1].iloc[:,59] # is_weekend\n",
    "weekday = df.loc[df.iloc[:,37]!=1].iloc[:,59] # ~is_weekend\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(weekend, weekday)\n",
    "\n",
    "print(f'T-statistic: {t_stat}, P-value: {p_value}')\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "664a90fa-3f7f-4ec5-bc23-17d5205ed686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: -9.86715672569703, P-value: 6.140743494103178e-23\n",
      "Reject the null hypothesis: There is a significant difference between the means.\n"
     ]
    }
   ],
   "source": [
    "# world vs not world\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "world = df.loc[df.iloc[:,17]==1].iloc[:,59] # Is data channel 'World'\n",
    "not_world = df.loc[df.iloc[:,17]!=1].iloc[:,59] # Is data channel not 'World'\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(world, not_world)\n",
    "\n",
    "print(f'T-statistic: {t_stat}, P-value: {p_value}')\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1ba4a-bf57-4086-9773-12f4ef83c4dc",
   "metadata": {},
   "source": [
    "## chi-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8976b999-f500-4dab-98db-055638c22768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square statistic: 990009.9531957436\n",
      "P-value: 1.0\n",
      "Degrees of freedom: 1050519\n",
      "Expected frequencies:\n",
      "[[0.0002018  0.0002018  0.0002018  ... 0.0002018  0.0002018  0.0002018 ]\n",
      " [0.0014378  0.0014378  0.0014378  ... 0.0014378  0.0014378  0.0014378 ]\n",
      " [0.00156392 0.00156392 0.00156392 ... 0.00156392 0.00156392 0.00156392]\n",
      " ...\n",
      " [0.00224498 0.00224498 0.00224498 ... 0.00224498 0.00224498 0.00224498]\n",
      " [0.00030269 0.00030269 0.00030269 ... 0.00030269 0.00030269 0.00030269]\n",
      " [0.00199274 0.00199274 0.00199274 ... 0.00199274 0.00199274 0.00199274]]\n",
      "Fail to reject the null hypothesis: There is no significant association between the variables.\n"
     ]
    }
   ],
   "source": [
    "# time delta\n",
    "\n",
    "time = df.iloc[:,0] # time delta\n",
    "shares = df.iloc[:,59] # shares\n",
    "\n",
    "contingency_table = pd.crosstab(time, shares)\n",
    "chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "print(f'Chi-square statistic: {chi2}')\n",
    "print(f'P-value: {p}')\n",
    "print(f'Degrees of freedom: {dof}')\n",
    "print('Expected frequencies:')\n",
    "print(expected)\n",
    "if p < 0.05:\n",
    "    print(\"Reject the null hypothesis: There is a significant association between the variables.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant association between the variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ce14e1f6-8bfc-465e-b225-24b34f68f691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a significant association between the variables [6, 8, 9, 11, 12, 13, 15, 16, 17, 19, 20, 21, 23, 27, 28, 30, 31, 32, 35, 37, 50, 54].\n"
     ]
    }
   ],
   "source": [
    "# time delta\n",
    "names = []\n",
    "shares = df.iloc[:,59] # shares\n",
    "\n",
    "for i in range(58):\n",
    "    time = df.iloc[:,i] # time delta    \n",
    "    \n",
    "    contingency_table = pd.crosstab(time, shares)\n",
    "    chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "    # print(f'Chi-square statistic: {chi2}')\n",
    "    # print(f'P-value: {p}')\n",
    "    # print(f'Degrees of freedom: {dof}')\n",
    "    # print('Expected frequencies:')\n",
    "    # print(expected)\n",
    "    if p < 0.05:\n",
    "        \n",
    "        names.append(i)\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "\n",
    "print(f'There is a significant association between the variables {names}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2a112faf-ba57-45e8-bb34-5ff6c96ebb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' num_hrefs',\n",
       " ' num_imgs',\n",
       " ' num_videos',\n",
       " ' num_keywords',\n",
       " ' data_channel_is_lifestyle',\n",
       " ' data_channel_is_entertainment',\n",
       " ' data_channel_is_socmed',\n",
       " ' data_channel_is_tech',\n",
       " ' data_channel_is_world',\n",
       " ' kw_max_min',\n",
       " ' kw_avg_min',\n",
       " ' kw_min_max',\n",
       " ' kw_avg_max',\n",
       " ' self_reference_min_shares',\n",
       " ' self_reference_max_shares',\n",
       " ' weekday_is_monday',\n",
       " ' weekday_is_tuesday',\n",
       " ' weekday_is_wednesday',\n",
       " ' weekday_is_saturday',\n",
       " ' is_weekend',\n",
       " ' min_positive_polarity',\n",
       " ' max_negative_polarity']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[df.columns[_] for _ in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da1b575-b61c-4375-a137-dff9bedf9e8d",
   "metadata": {},
   "source": [
    "## pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "aa760460-eaf7-4dad-99e6-2376d822ab94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: -0.04949730893119854\n",
      "P-value: 6.140743494102758e-23\n",
      "Reject the null hypothesis: There is a significant correlation between the variables.\n"
     ]
    }
   ],
   "source": [
    "world = df.iloc[:,17] # Is data channel 'World'\n",
    "shares = df.iloc[:,59]\n",
    "\n",
    "correlation_coefficient, p_value = stats.pearsonr(world, shares)\n",
    "\n",
    "print(f'Pearson correlation coefficient: {correlation_coefficient}')\n",
    "print(f'P-value: {p_value}')\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: There is a significant correlation between the variables.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant correlation between the variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f00ee-f4e8-4a7a-aca9-56115fe369bc",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "96e01c89-7b43-40e6-b1f5-41257e500f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 119062803.95594169\n",
      "R-squared: 0.024383250097501863\n",
      "Mean Absolute Error (MAE): 3132.666656218682\n",
      "Root Mean Squared Error (RMSE): 10911.590349529333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('OnlineNewsPopularity.csv')\n",
    "df.drop('url',axis = 1, inplace = True)\n",
    "# Prepare the data\n",
    "X = df.iloc[:,:59]  # feature\n",
    "y = df.iloc[:,59]  # Target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda9f16-8133-42e7-86ef-ddabb8430730",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfa50e-a07f-451a-a224-efc208cd0f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e75b306d-018c-4319-81bf-e0dbac249936",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91cbe76-739d-405f-bb65-b0666947dcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoScaling - Mean Squared Error (MSE): 75158789.33791369, R-squared (R²): 0.03240935707329606, Mean Absolute Error (MAE): 3062.3473230694267, Root Mean Squared Error (RMSE): 8669.416897226347\n",
      "StandardScaler - Mean Squared Error (MSE): 75157154.73270635, R-squared (R²): 0.03243040090220051, Mean Absolute Error (MAE): 3062.4843563491795, Root Mean Squared Error (RMSE): 8669.322622483625\n",
      "MinMaxScaler - Mean Squared Error (MSE): 75158789.3379143, R-squared (R²): 0.03240935707328818, Mean Absolute Error (MAE): 3062.347323075344, Root Mean Squared Error (RMSE): 8669.416897226381\n",
      "RobustScaler - Mean Squared Error (MSE): 75158789.31317444, R-squared (R²): 0.032409357391788074, Mean Absolute Error (MAE): 3062.3473164930974, Root Mean Squared Error (RMSE): 8669.416895799535\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('OnlineNewsPopularity.csv')\n",
    "df.drop('url',axis = 1, inplace = True)\n",
    "# Prepare the data\n",
    "X = df.iloc[:,:59]  # feature\n",
    "y = df.iloc[:,59]  # target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "scalers = {\n",
    "    'NoScaling': False,\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler()\n",
    "}\n",
    "\n",
    "# Evaluate each scaler\n",
    "results = {}\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    # Scale the data\n",
    "    if scaler:\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "    # Train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # Store the results\n",
    "    results[scaler_name] = {'MSE': mse, 'R²': r2, 'MAE': mae, 'RMSE': rmse }\n",
    "\n",
    "# Print the results\n",
    "for scaler_name, metrics in results.items():\n",
    "    print(f\"{scaler_name} - Mean Squared Error (MSE): {metrics['MSE']}, R-squared (R²): {metrics['R²']}, Mean Absolute Error (MAE): {metrics['MAE']}, Root Mean Squared Error (RMSE): {metrics['RMSE']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ed66d-c594-4598-b02c-0da8b319b35d",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c5ba97-32ae-47a2-8539-04407c06a33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 366488181.4604358\n",
      "R-squared: -1.4228669885387082\n",
      "Mean Absolute Error (MAE): 4214.352556458215\n",
      "Root Mean Squared Error (RMSE): 19143.88104487791\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('OnlineNewsPopularity.csv')\n",
    "df.drop('url',axis = 1, inplace = True)\n",
    "# Prepare the data\n",
    "X = df.iloc[:,:59]  # feature\n",
    "y = df.iloc[:,59]  # target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=True)\n",
    "\n",
    "# Add polynomial features\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97db605-a922-4a53-8b26-95d83defc6fe",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa9a425-8fc6-478f-958f-68c413a726bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cbb8d24-d051-4b7f-b2f6-56a67ed808e0",
   "metadata": {},
   "source": [
    "# 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af69c08b-6568-4ce7-9092-56b3fc0bad3f",
   "metadata": {},
   "source": [
    "## Absolute error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f15ede-b5f2-47c0-a464-17c21517d88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 137282131.68804097\n",
      "R-squared (R²): -0.01553127683088884\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('OnlineNewsPopularity.csv')\n",
    "df.drop('url',axis = 1, inplace = True)\n",
    "# Prepare the data\n",
    "X = df.iloc[:,:59]  # feature\n",
    "y = df.iloc[:,59]  # target\n",
    "\n",
    "# Define your custom loss function (e.g., Mean Absolute Percentage Error)\n",
    "def absolute_error_loss(beta, X, y):\n",
    "    y_pred = X @ beta\n",
    "    absolute_error = np.mean(np.abs(y - y_pred))\n",
    "    return absolute_error\n",
    "\n",
    "# Add an intercept term\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Optimize the custom loss function\n",
    "initial_beta = np.zeros(X.shape[1])\n",
    "result = minimize(absolute_error_loss, initial_beta, args=(X, y), method='BFGS')\n",
    "beta_optimized = result.x\n",
    "\n",
    "# Make predictions\n",
    "y_pred = X @ beta_optimized\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'R-squared (R²): {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a80f3-0e6f-40f7-9739-bc6cd8f892d8",
   "metadata": {},
   "source": [
    "## epsilon insensitive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3522abe-a46d-4b9c-bc91-a43cd7befab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 137321170.6605843\n",
      "R-squared (R²): -0.015820063850332744\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('OnlineNewsPopularity.csv')\n",
    "df.drop('url',axis = 1, inplace = True)\n",
    "# Prepare the data\n",
    "X = df.iloc[:,:59]  # feature\n",
    "y = df.iloc[:,59]  # target\n",
    "\n",
    "# Define your custom loss function (e.g., Mean Absolute Percentage Error)\n",
    "def epsilon_insensitive_loss(beta, X, y, epsilon=1.0):\n",
    "    y_pred = X @ beta\n",
    "    loss = np.maximum(0, np.abs(y - y_pred) - epsilon)\n",
    "    return np.mean(loss)\n",
    "\n",
    "# Add an intercept term\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Optimize the custom loss function\n",
    "initial_beta = np.zeros(X.shape[1])\n",
    "result = minimize(epsilon_insensitive_loss, initial_beta, args=(X, y), method='BFGS')\n",
    "beta_optimized = result.x\n",
    "\n",
    "# Make predictions\n",
    "y_pred = X @ beta_optimized\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'R-squared (R²): {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae8c78b-2dbb-4676-a614-45bc2e429e37",
   "metadata": {},
   "source": [
    "## Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fa64faf-0b66-4c93-8e2d-aa0c581ccfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 137328042.53102583\n",
      "R-squared (R²): -0.015870897846555376\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('OnlineNewsPopularity.csv')\n",
    "df.drop('url',axis = 1, inplace = True)\n",
    "# Prepare the data\n",
    "X = df.iloc[:,:59]  # feature\n",
    "y = df.iloc[:,59]  # target\n",
    "\n",
    "# Define your custom loss function (e.g., Mean Absolute Percentage Error)\n",
    "def huber_loss(beta, X, y, delta=1.0):\n",
    "    y_pred = X @ beta\n",
    "    residual = y - y_pred\n",
    "    condition = np.abs(residual) <= delta\n",
    "    squared_loss = 0.5 * residual**2\n",
    "    linear_loss = delta * (np.abs(residual) - 0.5 * delta)\n",
    "    return np.mean(np.where(condition, squared_loss, linear_loss))\n",
    "\n",
    "\n",
    "# Add an intercept term\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Optimize the custom loss function\n",
    "initial_beta = np.zeros(X.shape[1])\n",
    "result = minimize(huber_loss, initial_beta, args=(X, y), method='BFGS')\n",
    "beta_optimized = result.x\n",
    "\n",
    "# Make predictions\n",
    "y_pred = X @ beta_optimized\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'R-squared (R²): {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c22d5-4971-423d-9c10-f7061d3df114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
