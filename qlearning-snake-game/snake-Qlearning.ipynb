{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efcf5c06-973d-486c-ab86-95748187f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SnakeEnvironment as se\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85761220-bb92-4e61-82e9-d250b87f0996",
   "metadata": {},
   "source": [
    "based on the entire grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e11e54f3-3ac1-4223-98f9-cb137e8ab4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -18\n",
      "Episode 10000, Total Reward: 0\n",
      "Episode 20000, Total Reward: -5\n",
      "Episode 30000, Total Reward: -7\n",
      "Episode 40000, Total Reward: 10\n",
      "Episode 50000, Total Reward: -4\n",
      "Episode 60000, Total Reward: 8\n",
      "Episode 70000, Total Reward: -5\n",
      "Episode 80000, Total Reward: 9\n",
      "Episode 90000, Total Reward: -7\n",
      "Average Score over 100 test episodes: -62.38\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, grid_size=10, epsilon=1.0, alpha=0.1, gamma=0.99, epsilon_decay=0.995, num_episodes=1000):\n",
    "        self.env = env\n",
    "        self.grid_size = grid_size\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.num_episodes = num_episodes\n",
    "        \n",
    "        self.action_space_size = self.env.action_space.n  # 4 actions (UP, DOWN, LEFT, RIGHT)\n",
    "        self.q_table = np.zeros((grid_size * grid_size * grid_size * grid_size, self.action_space_size))  # Q-table\n",
    "        \n",
    "    def get_state_index(self, state):\n",
    "        \"\"\"\n",
    "        Converts the state (a 3D grid) into a unique integer index.\n",
    "        This method extracts the snake's head and the food's position from the grid.\n",
    "        \"\"\"\n",
    "        # Find the snake's head (first segment of the snake)\n",
    "        head_position = np.argwhere(state[:, :, 0] == 1)[0]  # Get the first occurrence of 1 (snake's head)\n",
    "        head_x, head_y = head_position[0], head_position[1]\n",
    "        \n",
    "        # Find the food position\n",
    "        food_position = np.argwhere(state[:, :, 0] == -1)[0]  # Get the first occurrence of -1 (food)\n",
    "        food_x, food_y = food_position[0], food_position[1]\n",
    "        \n",
    "        # Combine these positions into a unique integer index\n",
    "        return (head_x * self.grid_size + head_y) * (self.grid_size ** 2) + (food_x * self.grid_size + food_y)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action using epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        state_index = self.get_state_index(state)\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_space_size)  # Explore: Random action\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state_index])  # Exploit: Best known action\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Updates the Q-table using the Q-Learning formula.\n",
    "        \"\"\"\n",
    "        state_index = self.get_state_index(state)\n",
    "        next_state_index = self.get_state_index(next_state)\n",
    "        \n",
    "        max_next_q = np.max(self.q_table[next_state_index])  # Max Q-value for the next state\n",
    "        self.q_table[state_index, action] += self.alpha * (reward + self.gamma * max_next_q - self.q_table[state_index, action])\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the agent using Q-learning over multiple episodes.\n",
    "        \"\"\"\n",
    "        for episode in range(self.num_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Update Q-table with the reward and the next state\n",
    "                self.update_q_table(state, action, reward, next_state)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "            \n",
    "            # Decay epsilon to reduce exploration over time\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, 0.33)\n",
    "            \n",
    "            if episode % 10000 == 0:\n",
    "                print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    def evaluate(self, num_episodes=100):\n",
    "        \"\"\"\n",
    "        Evaluates the trained agent by running a number of test episodes.\n",
    "        \"\"\"\n",
    "        total_scores = []\n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            counter = 0\n",
    "            while not done :\n",
    "                counter += 1\n",
    "                if counter >= 1000: break\n",
    "                action = np.argmax(self.q_table[self.get_state_index(state)])  # Always exploit\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "            \n",
    "            total_scores.append(total_reward)\n",
    "        \n",
    "        average_score = np.mean(total_scores)\n",
    "        print(f\"Average Score over {num_episodes} test episodes: {average_score}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import gym  #\n",
    "    \n",
    "    # Initialize the environment and the QLearningAgent\n",
    "    env = se.SnakeEnv(grid_size=5) # used 5 to reduce calculations\n",
    "    agent = QLearningAgent(env, grid_size=5, epsilon=1.0, alpha=0.1, gamma=0.99, epsilon_decay=0.99995, num_episodes=100000)\n",
    "\n",
    "    # Train the agent\n",
    "    agent.train()\n",
    "    \n",
    "    # Evaluate the agent after training\n",
    "    agent.evaluate(num_episodes=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f73c7-45d7-483d-8b4d-306acfe22d3c",
   "metadata": {},
   "source": [
    "based on snake head location and food location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19dcec02-6cba-4431-8c6e-499c9e61d7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -50\n",
      "Episode 1000, Total Reward: -57\n",
      "Episode 2000, Total Reward: -449\n",
      "Episode 3000, Total Reward: -512\n",
      "Episode 4000, Total Reward: -43\n",
      "Episode 5000, Total Reward: -51\n",
      "Episode 6000, Total Reward: -48\n",
      "Episode 7000, Total Reward: -16\n",
      "Episode 8000, Total Reward: -44\n",
      "Episode 9000, Total Reward: -129\n",
      "Average Score over 100 test episodes: -999.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, grid_size=10, epsilon=1.0, alpha=0.1, gamma=0.99, epsilon_decay=0.995, num_episodes=1000):\n",
    "        self.env = env\n",
    "        self.grid_size = grid_size\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.num_episodes = num_episodes\n",
    "        \n",
    "        self.action_space_size = self.env.action_space.n  # 4 actions (UP, DOWN, LEFT, RIGHT)\n",
    "        self.q_table = np.zeros((grid_size * grid_size * grid_size * grid_size, self.action_space_size))  # Q-table\n",
    "        \n",
    "    def get_state(self, state):\n",
    "        \"\"\"\n",
    "        Converts the state (snake grid) into a simple tuple (head_x, head_y, food_x, food_y).\n",
    "        Extracts the snake's head position and the food position from the state.\n",
    "        \"\"\"\n",
    "        # Get the snake's head position (first occurrence of '1' in state)\n",
    "        head_x, head_y = self.env.snake[0]\n",
    "        \n",
    "        food_x, food_y = self.env.food\n",
    "        \n",
    "        return (head_x, head_y, food_x, food_y)\n",
    "    \n",
    "    def get_state_index(self, state):\n",
    "        \"\"\"\n",
    "        Converts the state (head_x, head_y, food_x, food_y) into a unique integer index.\n",
    "        \"\"\"\n",
    "        head_x, head_y, food_x, food_y = state\n",
    "        \n",
    "        # Combine these positions into a unique integer index\n",
    "        return (head_x * self.grid_size + head_y) * (self.grid_size ** 2) + (food_x * self.grid_size + food_y)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action using epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        state = self.get_state(state)  # Convert state to (head_x, head_y, food_x, food_y)\n",
    "        state_index = self.get_state_index(state)\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_space_size)  # Explore: Random action\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state_index])  # Exploit: Best known action\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Updates the Q-table using the Q-Learning formula.\n",
    "        \"\"\"\n",
    "        state = self.get_state(state)  # Convert state to (head_x, head_y, food_x, food_y)\n",
    "        next_state = self.get_state(next_state)  # Convert next_state to (head_x, head_y, food_x, food_y)\n",
    "        \n",
    "        state_index = self.get_state_index(state)\n",
    "        next_state_index = self.get_state_index(next_state)\n",
    "        \n",
    "        max_next_q = np.max(self.q_table[next_state_index])  # Max Q-value for the next state\n",
    "        self.q_table[state_index, action] += self.alpha * (reward + self.gamma * max_next_q - self.q_table[state_index, action])\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the agent using Q-learning over multiple episodes.\n",
    "        \"\"\"\n",
    "        for episode in range(self.num_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Update Q-table with the reward and the next state\n",
    "                self.update_q_table(state, action, reward, next_state)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "            \n",
    "            # Decay epsilon to reduce exploration over time\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, 0.2)\n",
    "            \n",
    "            if episode % 1000 == 0:\n",
    "                print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    def evaluate(self, num_episodes=100):\n",
    "        \"\"\"\n",
    "        Evaluates the trained agent by running a number of test episodes.\n",
    "        \"\"\"\n",
    "        total_scores = []\n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            state = self.get_state(state)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            counter = 0\n",
    "            while not done :\n",
    "                counter += 1\n",
    "                if counter >= 1000: break\n",
    "                action = np.argmax(self.q_table[self.get_state_index(state)])  # Always exploit\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = self.get_state(next_state)\n",
    "                total_reward += reward\n",
    "            \n",
    "            total_scores.append(total_reward)\n",
    "        \n",
    "        average_score = np.mean(total_scores)\n",
    "        print(f\"Average Score over {num_episodes} test episodes: {average_score}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import gym\n",
    "    # Initialize the environment and the QLearningAgent\n",
    "    env = se.SnakeEnv(grid_size=10)\n",
    "    agent = QLearningAgent(env, grid_size=10, epsilon=1.0, alpha=0.1, gamma=0.99, epsilon_decay=0.995, num_episodes=10000)\n",
    "\n",
    "    # Train the agent\n",
    "    agent.train()\n",
    "    \n",
    "    # Evaluate the agent after training\n",
    "    agent.evaluate(num_episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d92cb6f-4cd6-4c99-8378-341e866d0043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.snake[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d7145-1ec7-4abe-ab10-25d85943aaeb",
   "metadata": {},
   "source": [
    "focus on the difference between snake head and food locations (we do this to reduce q-table size for faster calculations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdf4361f-94d5-4edd-ab07-64a3a8633d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -27\n",
      "Episode 10000, Total Reward: -185\n",
      "Episode 20000, Total Reward: -22\n",
      "Episode 30000, Total Reward: -1072\n",
      "Episode 40000, Total Reward: -32\n",
      "Episode 50000, Total Reward: -33\n",
      "Episode 60000, Total Reward: -33\n",
      "Episode 70000, Total Reward: -44\n",
      "Episode 80000, Total Reward: -91\n",
      "Episode 90000, Total Reward: -114\n",
      "Average Score over 100 test episodes: -899.63\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, grid_size=10, epsilon=1.0, alpha=0.1, gamma=0.99, epsilon_decay=0.995, num_episodes=1000):\n",
    "        self.env = env\n",
    "        self.grid_size = grid_size\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.num_episodes = num_episodes\n",
    "        \n",
    "        self.action_space_size = self.env.action_space.n  # 4 actions (UP, DOWN, LEFT, RIGHT)\n",
    "        # Q-table has fewer states because we're using relative positions\n",
    "        self.q_table = np.zeros((grid_size * grid_size * 4, self.action_space_size))  # 4 directions for food (up, down, left, right)\n",
    "        \n",
    "    def get_relative_food_position(self, head_x, head_y, food_x, food_y):\n",
    "        \"\"\"\n",
    "        Returns the relative direction of the food with respect to the snake's head:\n",
    "        - 'up', 'down', 'left', or 'right'\n",
    "        \"\"\"\n",
    "        if food_x < head_x:\n",
    "            return \"left\"\n",
    "        elif food_x > head_x:\n",
    "            return \"right\"\n",
    "        elif food_y < head_y:\n",
    "            return \"up\"\n",
    "        elif food_y > head_y:\n",
    "            return \"down\"\n",
    "        return \"none\"  # Should never reach here if food and head are not the same position\n",
    "    \n",
    "    def get_state(self, state):\n",
    "        \"\"\"\n",
    "        Convert the state (snake grid) into a simple tuple (head_x, head_y, relative_food_direction).\n",
    "        Extracts the snake's head position and the food position from the state.\n",
    "        \"\"\"\n",
    "        # Get the snake's head position (first occurrence of '1' in state)\n",
    "        head_x, head_y = self.env.snake[0]\n",
    "        \n",
    "        # Get the food position\n",
    "        food_x, food_y = self.env.food\n",
    "        \n",
    "        # Get relative food position\n",
    "        food_direction = self.get_relative_food_position(head_x, head_y, food_x, food_y)\n",
    "        \n",
    "        return (head_x, head_y, food_direction)\n",
    "    \n",
    "    def get_state_index(self, state):\n",
    "        \"\"\"\n",
    "        Converts the state (head_x, head_y, food_direction) into a unique integer index.\n",
    "        \"\"\"\n",
    "        head_x, head_y, food_direction = state\n",
    "        \n",
    "        # Map food direction to integer\n",
    "        food_direction_map = {\"left\": 0, \"right\": 1, \"up\": 2, \"down\": 3}\n",
    "        food_direction_index = food_direction_map[food_direction]\n",
    "        \n",
    "        # Combine these positions into a unique integer index\n",
    "        return (head_x * self.grid_size + head_y) * 4 + food_direction_index\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action using epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        state = self.get_state(state)  # Convert state to (head_x, head_y, food_direction)\n",
    "        state_index = self.get_state_index(state)\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_space_size)  # Explore: Random action\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state_index])  # Exploit: Best known action\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Updates the Q-table using the Q-Learning formula.\n",
    "        \"\"\"\n",
    "        state = self.get_state(state)  # Convert state to (head_x, head_y, food_direction)\n",
    "        next_state = self.get_state(next_state)  # Convert next_state to (head_x, head_y, food_direction)\n",
    "        \n",
    "        state_index = self.get_state_index(state)\n",
    "        next_state_index = self.get_state_index(next_state)\n",
    "        \n",
    "        max_next_q = np.max(self.q_table[next_state_index])  # Max Q-value for the next state\n",
    "        self.q_table[state_index, action] += self.alpha * (reward + self.gamma * max_next_q - self.q_table[state_index, action])\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the agent using Q-learning over multiple episodes.\n",
    "        \"\"\"\n",
    "        for episode in range(self.num_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Update Q-table with the reward and the next state\n",
    "                self.update_q_table(state, action, reward, next_state)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "            \n",
    "            # Decay epsilon to reduce exploration over time\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, 0.2)\n",
    "            \n",
    "            if episode % 10000 == 0:\n",
    "                print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    def evaluate(self, num_episodes=100):\n",
    "        \"\"\"\n",
    "        Evaluates the trained agent by running a number of test episodes.\n",
    "        \"\"\"\n",
    "        total_scores = []\n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            state = self.get_state(state)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            counter = 0\n",
    "            while not done:\n",
    "                counter += 1\n",
    "                if counter >= 1000: break\n",
    "                action = np.argmax(self.q_table[self.get_state_index(state)])  # Always exploit\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = self.get_state(next_state)\n",
    "                total_reward += reward\n",
    "            \n",
    "            total_scores.append(total_reward)\n",
    "        \n",
    "        average_score = np.mean(total_scores)\n",
    "        print(f\"Average Score over {num_episodes} test episodes: {average_score}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import gym\n",
    "    # Initialize the environment and the QLearningAgent\n",
    "    env = se.SnakeEnv(grid_size=10)\n",
    "    agent = QLearningAgent(env, grid_size=10, epsilon=1.0, alpha=0.1, gamma=0.99, epsilon_decay=0.995, num_episodes=100000)\n",
    "\n",
    "    # Train the agent\n",
    "    agent.train()\n",
    "    \n",
    "    # Evaluate the agent after training\n",
    "    agent.evaluate(num_episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3599eca-9742-4971-a5de-e86265809b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.q_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb28651-3f5f-4c7d-845e-ad264493aede",
   "metadata": {},
   "source": [
    "obviously the first one is the best model because it has more information about the space, but the last one is the fastest one to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc28b2a9-d141-4f0c-9de1-774501ac9ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
